---
title: Séries d'ateliers R - UQAM - BIA3010
subtitle: Atelier 3 - Analyse de variance et régression simple
author:
  - name: | 
        Maxime Fraser Franco : 
    affiliation: Département des Sciences Biologiques & Centre de la Science de la Biodiversité du Québec, Université du Québec à Montréal
    email: fraser_franco.maxime@courrier.uqam.ca
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
    highlight: zenburn
    theme: flatly
    df_print: paged
    #code_folding: hide
#bibliography: refs-MLMM.bib
#nocite: |
#  @McElreath2020
#  @Burkner2017
#  @Burkner2018
#  @FraserFranco.etal2022
#  @Piironen.Vehtari2017
#  @aczelDiscussionPointsBayesian2020
#  @kruschkeBayesianAnalysisReporting2021
#  @Vehtari.etal2017
---

<br>

# Sommaire

Dans cet atelier, nous allons voir comment utiliser R pour effectuer deux types d'analyses classiques en biologie, mais aussi même dans plusieurs sciences en général. Ces analyses sont:

- L'analyse de variance (ANOVA)
- La régression linéaire simple

Ces deux analyses sont utilisées pour différents cas de figure et dépendent du "design" expérimental que vous avez fait. Nous allons voir comment reconnaître ces cas de figure pour rapidement évaluer le type d'analyse nécessaire pour répondre à nos questions.

**À noter**: le présent document est en format .html et a été généré avec [R Markdown](https://rmarkdown.rstudio.com/). C'est simplement une façon de générer des documents beaux pour expliquer du code. Ainsi, vous pouvez copier-coller le code de chaque cellule dans une session R sur votre ordinateur personnel et produire l'ensemble des analyses présentées dans le document.

<br>




# Préparer notre session

Pour initier notre session de programmation dans R, il nous faudra faire 2 étapes essentielles. La première sera de charger les librairies nécessaires pour effectuer nos analyses, puis ensuite d'importer nos données dans la session pour commencer à les manipuler et les modéliser.


## Charger les librairies

Voici les librairies que nous utiliserons:

- Package [`ggplot2`](https://ggplot2.tidyverse.org/) pour faire de magnifiques graphiques
- Package [`boot`](https://cran.r-project.org/web/packages/boot/boot.pdf) pour faire des simulations

Chargons donc nos librairies. Si vous ne les avez pas installées, vous devez tout dabord le faire en faisant la commande `install.packages("librairie")`.
```{r}

# Définir le vecteur des librairies à charger
packages <- c("ggplot2", "boot")

# Charger toutes les librairies d'un coup
lapply(packages, library, character.only = TRUE)

```


## Importer les données

Dépendemment de comment vous utilisez R, vous devrez tout d'abord peut-être devoir "set" votre working directory avec `setwd("votre/chemin/dans/windows/ou/mac")`

Par contre, si vous travaillez à l'échelle du projet dans Rstudio comme nous avons vu à l'atelier 2, alors pas besoin de `setwd()`. Vous pourrez confirmer que vous êtes déjà dans le bon chemin (dossier) en faisant la commande `getwd()`.

Nous allons travailler avec deux bases de données différentes. La première consiste en des données de capture de micro-mammifères à travers 80 fragments forestiers, et l'autre des données environnementales qui caractérisent les 80 fragments, étant la superficie des fragments ainsi que le type de forêt dans lequel le fragment a été échantillonné. Les trois types de forêts sont :

- des fragments en forêt mature
- des fragments en forêt avec coupes partielles
- des fragments dans des forêts ou il y a eu coupe totale

```{r}
# Préparons le chemin vers nos données
chemin <- file.path(getwd(), "2-Data")

# Données de capture d'espèces dans différents fragments
donnees_capture <- read.table(
    file = file.path(chemin, "donnees-captures.csv"),
    header = TRUE, sep = ";", dec = "."
)

# Données de superficie de fragments forestiers selon le type de forêt
donnees_fragments <- read.table(
    file = file.path(chemin, "donnees-fragments.csv"),
    header = TRUE, sep = ";", dec = "."
)
```

<br>




# Visualiser nos données bruts

```{r}
str(donnees_capture)
head(donnees_capture)
```

```{r}
str(donnees_fragments)
head(donnees_fragments)
```

<br>




# Préparer une table de données pour les analyses

Nous allons tout d'abord créer une table synthétique qui combine les données de captures et de types de fragments forestiers. Notre objectif est de créer une table où on retrouve comme information :

- le nombre d'espèces capturées par fragment
- le nombre de captures par fragment
- la superficie du fragment
- le type de fragment

Puisque nous avons 80 fragments, nous nous attendons à ce que cette table soit de 80 lignes de longueur puisque nous avons 1 mesure par fragment forestier.

Nous allons en premier lieu calculer le nombre de captures par fragment forestier en utilisant la fonction `aggregate()`. L'argument `FUN = length` réfère à la fonction qui est utilisée pour faire le calcul à travers les fragments.
```{r}
nbr_captures <- aggregate(
    donnees_capture$capture,
    list(donnees_capture$fragment),
    FUN = length
)

# On renomme les colones
names(nbr_captures) <- c("fragment", "nbr_capture")
```

On va ensuite créer un objet sous forme de `data.frame` où on conserve comme information seulement la liste des espèces trouvées par fragment forestier. Visualisons à quoi ressemble notre table.
```{r}
donnees_capture1 <- unique(donnees_capture)
head(donnees_capture1)
```

À partir de l'objet créé `donnees_capture1`, on recense le nombre d'espèces par fragment. À noter que notre unité d'échantillonage est le fragment qui sont des réplicas pour le type de fragment forestier, soit, coupe totale, coupe partielle, et la forêt mature. 
```{r}
nbr_especes <- aggregate(
    donnees_capture1$capture,
    by = list(donnees_capture1$fragment),
    FUN = length
)
names(nbr_especes)  <- c("fragment", "nbr_especes")

nbr_especes

```

Finalement, la dernière étape consiste à combiner les données que nous avons créé pour avoir une table synthétique prête à être utilisée pour nos analyses.
```{r}
nbrs <- merge(nbr_captures, nbr_especes, by = "fragment")
data_final <- merge(donnees_fragments, nbrs, by = "fragment")
```

On peut visualiser la structure de cette table pour bien la comprendre.
```{r}
str(data_final)
head(data_final)
``` 

La table consiste en une liste de 80 fragments forestiers issus de forêts matures, forêts à coupe partielle, et forêt à coupe totale. Notez que nous avons la superficie de chaque fragment forestier échantillonné. Finalement, l'information cruciale consiste en le nombre de captures qui ont été effecutées à chaque fragment, ainsi que le nombre d'espèces qui a été observé à ces fragments. À la lumière de ces informations, quelles questions écologiques pourrions-nous nous poser et quel type d'analyse pourrions-nous faire pour répondre à nos questions?

<br>




# Exploration de données

L'exploration de données est une étape importante du cycle d'un projet d'analyse de données car elle nous permet de bien comprendre la structure de nos données ainsi que ses attributs.


## Structure de la table de données

Commençons par vérifier les informations de base de notre table
```{r}
str(data_final)
```

Vérifions ensuite un sommaire des données
```{r}
summary(data_final)
```


## Visualisation des données

Une des étapes clés lorsqu'il est temps d'explorer nos données est de produire des figures pour décrire nos variables. Ici, notre variable d'intérêt est le nombre d'espèces trouvées. Puisque nous nous intéressons à savoir si le nombre d'espèces varie entre les types de forêt, nous allons produire des figures pour chaque type afin de les comparer visuellement.

Nous allons ainsi visualiser la distribution du nombre d'espèces dans nos fragments forestiers à l'aide d'histogrammes.

```{r fig.width = 12, fig.length = 6}
labels <- labeller(
  variable =
  c("partielle" = "Forêt à coupe partielle",
    "totale" = "Forêt à coupe totale",
    "mature" = "Forêt mature")
)

labels <- c(
    "partielle" = "Forêt à coupe partielle",
    "totale" = "Forêt à coupe totale",
    "mature" = "Forêt mature"
)

fig <- ggplot(data = data_final, aes(x = nbr_especes)) +
  geom_histogram(col = "black", fill = "gray", bins = 10) +
  xlab("\nNombre d'espèces") +
  ylab("Fréquence\n") +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  #facet_grid(~ type, labeller = labels) +
  facet_wrap(~ type, labeller = as_labeller(labels)) +
  theme_bw(base_size = 12) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12))

fig
```

On voit que la distribution du nombre d'espèces varie d'un type de forêt à l'autre.

Une manière de décrire la distribution de nos données est aussi d'utiliser une mesure de tendance centrale comme la moyenne. Nous avons déjà vu que la distribution du nombre d'espèce varie d'une forêt à l'autre, mais qu'en est-il de leur moyenne? Voyons voir.

On commence par vérifier la moyenne de tous les échantillons confondus.
```{r}
mean(data_final$nbr_especes)
```

Qu'en est-il de la moyenne par type de forêt?
```{r}
aggregate(
  nbr_especes ~ type,
  data = data_final,
  FUN = mean
)
```

Rapidement, il semble que nos moyennes diffèrent par rapport à la forêt à coupe totale. Par contre, pour confirmer que cela est le cas, il va falloir le tester "statistiquement". Nous allons voir comment faire cela dans la prochaine section.

<br>




# L'analyse de variance (ANOVA)

L'analyse de variance est une méthode de choix lorsqu'il s'agit de répondre à une question scientifique comme la nôtre. Rappelez-vous qu'on s'intéresse à savoir si le nombre d'espèces qu'on trouve diffèrent entre différents types de forêts. Si on observe des différences, il est possible de supposer que le type de forêt influence le nombre d'espèces qu'on y retrouve, mais il faut faire attention pour ne pas établir de liens de causalité.

Avant de procéder à notre analyse, il est important de connaître les étapes clés lorsqu'on fait un modèle linéaire, étant de :

**1.** Produire le modèle

**2.** Vérifier les postulats (suppositions) de base

si l'étape 2 échoue, on retourne à 1 et on ajuste le modèle

**3.** Quand on réussi l'étape 2, on produit la table des estimés

**4.** Produire un graphique

Ces étapes devraient être faites dans cet ordre et répétées autant que nécessaire jusqu'à ce qu'on soit confiant que notre modèle est juste et qu'il nous donne une information qui fait du sens scientifiquement.


## Produire le modèle

Nous commençons par produire le modèle en utilisant la fonction `lm()`, pour "linear model". Rappelez-vous que l'ANOVA fait partie de la grande famille des modèles linéaires, qui inclus la régression.

```{r}
# Avant tout, il faut que la colone "type" soit un facteur
data_final$type <- as.factor(data_final$type)

# Comme l'anova est un modèle linéaire, on utilise la fonction "lm()"
modele <- lm(nbr_especes ~ type, data = data_final)
```


## Vérifier les postulats de base

Les vérifications pour le respect des postulats de base se font de manière visuelle. Il existe des méthodes statistiques pour vérifier les postulats, mais la méthode graphique est largement supérieure car elle est + nuancée.

La première condition est que les données sont indépendantes. Il n'y a pas de vérification visuelle pour cela et vous devez normalement en tenir compte selon votre design expérimental. Les autres conditions vont comme suit:

on vérifie la distribution des résidus pour s'assurer qu'elle est Gaussienne (normale).

```{r}
# On extrait les résidus
residus <- resid(modele)

# Vérifier la normalité des résidus
hist(
  residus,
  xlab = "Résidus du modèle",
  ylab = "Nombre d'observations",
  main = "", col = "darkgray", cex.lab = 1.5
)
```

On s'assure ensuite que nos variances intra-groupe sont homogènes. Cela veut dire qu'on veut que les résidus pour chaque groupe soient distribués de façon homogène, ou encore juste que les résidus soient environ pareils à travers les groupes. 

```{r}
boxplot(
  residus ~ data_final$type,
  ylab = "Résidus du modèle",
  xlab = "Type de forêt",
  col = "darkgray"
)
```


## Produire la table des estimés

Une autre chose qu'on faire pour commencer est de visualiser la table d'ANOVA du modèle. On voit que la valeur de notre test de F est assez élevée et la valeur p est basse, indiquant qu'il existe au moins une différence entre deux types de forêts dans leur nombre d'espèces.

```{r}
anova(modele)
```

Une autre méthode est de visualiser le sommaire du modèle linéaire avec la fonction `summary()`. Cette fonction va nous retourner la valeur des coefficients prédits par le modèle, nous permettant de savoir s'il existe des différences dans le nombre d'espèces entre nos types de forêts.

```{r}
summary(modele)
```

Ici, les informations les + importantes sont les estimés (`Estimate`), le test de T (`t value`), et le R2 ajusté.

Lorsqu'on fait un modèle de type ANOVA et qu'on utilise `summary()`, la première ligne de la table (`Intercept`) nous indique la le nombre d'espèces moyen prédit par notre modèle pour le groupe de référence, ici étant la forêt mature. Les autres valeurs correspondent à la différence par rapport au groupe de référence. Ainsi, la valeur de `-0.1543` pour la forêt à coupe partielle indique que celle-ci a `-0.1543` espèces par rapport à la forêt mature, soit, `6.8065 - 0.1543 = 6.6522` espèces.

Les tests de T nous indiquent si la valeur moyenne estimée diffère de 0 pour le groupe de référence (forêt mature), alors que pour les autres, c'est si la différence est significativement différente de 0. En soi, le test de T n'est pas très informatif car il ne fait pas de comparaison entre les groupes.

Le R carré donne une information cruciale car ici, il nous indique que 19% de la variation dans le nombre d'espèces échantillonné est expliquée par le type de forêt. Ça veut dire que le type de forêt à lui seul est un facteur de haute importance pour expliquer la distribution des espèces.


## Tests post-hoc

On peut finalement directement comparer les moyennes entre nos types de forêt pour savoir exactement où se trouvent les différences. On utilise typiquement des test de Tukey pour faire ça.

```{r}
TukeyHSD(aov(modele), ordered = TRUE)
```

Une bien meilleure façon est de faire des simulations avec du bootstrapping où on produis 1000x notre modèle et qu'on extrait l'information.

```{r}
# Définir une fonction pour produire le modèle et extraire les moyennes
fit_and_extract_means <- function(data, indices) {
  sampled_data <- data_final[indices, ]
  model <- lm(nbr_especes ~ type, data = sampled_data)
  means <- tapply(predict(model), sampled_data$type, mean)
  return(means)
}

# Nombre de simulations
n_replicates <- 1000

# Faire le bootstrapping
set.seed(123) # pour reproduire les résultats chaque fois
boot_results <- boot(
  data_final[, c(3, 5)],
  fit_and_extract_means,
  R = n_replicates
)

# Extraire les résultats du bootstrap
# c'est une matrice à 3 colones où chaque colone représente un milieu
# On a donc 1000 moyennes simulées par type de forêt
bootstrap_matrix <- boot_results$t

# On calcule la moyenne et les intervalles de confiance à 95%
# pour chaque type de forêt
means <- colMeans(bootstrap_matrix)
ci_low <- apply(bootstrap_matrix, 2, function(x) quantile(x, 0.025))
ci_high <- apply(bootstrap_matrix, 2, function(x) quantile(x, 0.975))

# On produit une table de données pour montrer les résultats
results_df <- data.frame(
  type = levels(data_final$type),
  mean = means,
  ci_low = ci_low,
  ci_high = ci_high
)

# Créer le graphique
ggplot(results_df, aes(x = type, y = mean)) +
  geom_pointrange(
    aes(ymin = ci_low, ymax = ci_high),
    size = 0.8,
    position = position_dodge(width = 0.3)
  ) +
  scale_y_continuous(breaks = seq(0, 8, 2), limits = c(0, 8)) +
  labs(x = "\nNombre d'espèces moyen",
       y = "Type de forêt\n") +
  theme_bw(base_size = 12) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12))
```

## Produire le graphique

Le graphique que nous avons utilisé précédemment aurait été parfait pour représenter notre modèle. Toutefois, puisqu'il y a peu de chances que vous fassiez des "bootstrap", une version simplifié pourrait être de calculer les moyennes prédites par votre modèle pour chaque type de forêt, et faire le graphique des prédictions avec l'erreur. Voici un exemple.

On commence par préparer une table de données pour produire notre graphique. Il s'agit tout d'abord de recréer un jeu de données et d'extraire ensuite les prédictions du modèle pour chaque type de forêt.

```{r}
# Jeu de données à partir des données originales
new_dat <- data.frame(type = levels(data_final$type))

# Matrice des coefficients du modèle
mm <- model.matrix(
  ~ type,
  new_dat
)

# Estimer les valeur prédites pour chaque forêt
fitted_values <- mm %*% coef(modele)

# Calculer les intervalles de confiance
pvar <- diag(mm %*% tcrossprod(vcov(modele), mm))
tvar <- pvar + var(resid(modele))
```

On peut ensuite générer une table avec la moyennne et les intervalles de confiance pour chaque type de forêt, et on utilise cette table pour faire notre graphique. Comme vous pouvez voir, ça nous donne exactement le même graphique que tout à l'heure. Toutefois, celui-ci provient directement des coefficients de notre modèle avec son erreur, sans qu'on aille simulé quoi que ce soit.

```{r}
# Générer une table pour produire facilement le graphique
table <- data.frame(
  type = new_dat$type,
  nmbr = fitted_values,
  lower = fitted_values - 1.96 * sqrt(pvar),
  upper = fitted_values + 1.96 * sqrt(pvar)
)

# Produire le graphique des prédictions
ggplot(table, aes(x = type, y = nmbr)) +

# On commence par ajouter les données bruts
  geom_jitter(
    data = data_final, aes(x = type, y = nbr_especes),
    shape = 16, color = "dodgerblue", size = 2.5,
    position = position_jitter(0.2)
  ) +

# Prédictions du modèle
  geom_pointrange(
    aes(ymin = lower, ymax = upper),
    size = 0.8,
    position = position_dodge(width = 0.3)
  ) +

# Paramètres graphiques
  scale_y_continuous(breaks = seq(0, 12, 4), limits = c(0, 12.5)) +
  xlab("\nType de forêt") +
  ylab("Nombre d'espèces moyen prédit\n") +
  theme_bw(base_size = 12) +
  theme(
    panel.grid = element_blank(),
    strip.text = element_text(size = 12)
  )
```

Finalement, une méthode encore + simple que vous pouvez utiliser est de produire un diagramme en boîte à moustaches de vos données bruts. Cependant, il est à noter que cette représentation visuelle est sur les données bruts, et elle ne contient donc pas toute l'information que notre modèle contenait. C'est toutefois la méthode la plus simple et directe de représenter nos données.

```{r}
ggplot(data = data_final, aes(x = type, y = nbr_especes)) +

# On ajoute le boxplot
  geom_boxplot(color = "black", fill = "lightgray") +

# On ajoute les données bruts
  geom_jitter(
    data = data_final, aes(x = type, y = nbr_especes),
    shape = 16, color = "dodgerblue", size = 2.5,
    position = position_jitter(0.2)
  ) +

# Paramètres graphiques
  scale_y_continuous(breaks = seq(0, 12, 4), limits = c(0, 12.5)) +
  xlab("\nType de forêt") +
  ylab("Nombre d'espèces capturées\n") +
  theme_bw(base_size = 12) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12))
```

# Conclusions de l'ANOVA

À la lumière de nos résultats, nous pouvons conclure qu'il existe bel et bien une différence dans le nombre d'espèces capturées entre les types de forêt. Nous avons pu voir qu'il n'y a pas de différence entre les forêts à coupes partielles et les forêts matures. Toutefois, le nombre d'espèces moyen capturées dans les fragments de forêts à coupe totale diffèrent significativement de ceux des deux autres forêts.

<br>




# Pause

respirons...

<br>




# La régression linéaire simple

La régression linéaire simple est une technique classique qui constitue la base de multiples techniques + complexes ainsi que celles développées en intelligence artificielle.

Ici, nous allons continuer notre exploration des données de fragments forestiers pour en nous posant une nouvelle question. Comme vous vous rappelez, nous avons des données de superficie pour chaque fragment. Il est bien connu en écologie que la superficie d'un milieu peut influencer le nombre d'espèces qu'on y retrouve.

Typiquement, on assume que + la superficie est grande, plus un habitat peut supporter un nombre d'espèces élevé. Nous allons donc tester cette hypothèse à l'aide d'un régression linéaire où notre variable x sera la superficie des fragments.

Les étapes sont les mêmes que pour l'ANOVA. Commençons!

## Produire le modèle

Nous allons tout d'abord commencer par centrer et réduire ("standardiser") notre variable superficie. Nous allons voir pourquoi plus loin. L'idée est de soustraire la superficie moyenne à chaque observation et ensuite diviser la valeur par l'écart-type de la superficie. Nos observations de superficie seront alors transformées en unitées d'écart-type. Les valeurs positives indiquent des superficies supérieures à la moyenne et les valeurs négatives indiquent des superficies inférieures à la moyenne.

On utilise la fonction `scale()` pour faire cela et effectue la transformation sur le vecteur de superficie. On assigne la transformation à la table en créant une nouvelle colonne appelée `superficie_cr`.

```{r}
data_final$superficie_cr <- scale(data_final$superficie)
```

On peut maintenant produire le modèle linéaire.

```{r}
# Comme l'anova est un modèle linéaire, on utilise la fonction "lm()"
mod_reg <- lm(nbr_especes ~ superficie_cr, data = data_final)
```


## Vérifier les postulats de base

Pour la régression les postulats de base sont juste un peu différents:

1. Les échantillons sont indépendants
2. La relation entre y et x est linéaire
3. Les résidus sont Gaussiens
4. Les résidus sont homogènes autour de la droite de régression

On vérifie d'abord que les résidus sont Gaussiens.

```{r}
# On extrait les résidus
resid_reg <- resid(mod_reg)

# Vérifier la normalité des résidus
hist(
  resid_reg,
  xlab = "Résidus du modèle",
  ylab = "Nombre d'observations",
  main = "", col = "darkgray", cex.lab = 1.5
)
```

On vérifie ensuite si nos résidus sont homogènes le long de nos valeurs prédites. On veut que les observations soient distribuées le long de la droite pointillée de façon relativement homogène. Si on ne detecte pas de patron clair, notre modèle est donc bon et les résidus sont OK!

```{r}
plot(
  resid_reg ~ fitted(mod_reg),
  xlab = "Valeurs prédites par le modèle",
  ylab = "Résidus du modèle",
  pch = 19
)
abline(
  h = 0,
  col = "dodgerblue",
  lty = "dashed",
  lwd = 2
)
```


## Produire la table des estimés

Maintenant que nous savons que l'ajustement de notre modèle est OK, nous allons visualiser le sommaire du modèle linéaire avec la fonction `summary()`.

```{r}
summary(mod_reg)
```

La sortie est un peu différente de ce que nous avons vu tout à l'heure. Les paramètres importants demeurent les mêmes que pour l'ANOVA, mais ils ne s'interprètent pas de la même façon.

Tout d'abord, l'ordonnée à l'origine `Intercept` nous indique la valeur moyenne d'espèces observées pour une superficie moyenne. Ceci est important à comprendre et nous ramène à la raison pour laquelle nous avons centré et réduit. Normalement, une ordonnée à l'origine correspond à la moyenne de y lorsque x est égal à 0. Toutefois, cela ne fait pas de sens dans notre cas car il est impossible d'avoir un fragment forestier avec une superficie de 0. C'est pourquoi nous avons centré réduit notre superficie pour que l'interprétation de l'ordonnée à l'origine se fasse à partir d'une superficie moyenne.

Ensuite, la valeur `Estimate` pour la superficie_cr nous indique la direction de la relation entre le nombre d'espèces et la superficie ainsi que sa force. Comme vous pouvez voir, pour chaque unité d'écart-type de superficie, il semble que nous avons une augmentation de 1.59 espèces, ce qui est grand!

Le test de t nous indique simplement si la valeur de l'ordonnée à l'origine ainsi que celle de la pente est significativement différente de 0. Dans les deux cas, la statistique est élevée (`t value`) et la valeur p est sous le seuil de 0.05. Il ya donc une relation positive et significative entre le nombre d'espèces et la superficie.

Finalement, nous voyons que le R carré ajusté est de 34%, ce qui est très important. Encore une fois, cela veut dire que 34% de notre variation en nombre d'espèces est attribuable à la superficie des fragments forestiers.


## Produire le graphique

On commence par produire les prédictions de notre modèle pour faire le graphique.

```{r}
# Jeu de données à partir des données originales
new_dat <- data.frame(
  superficie = seq(
    min(data_final$superficie_cr),
    max(data_final$superficie_cr),
    length.out = 100
  )
)

# Matrice des coefficients du modèle
mm <- model.matrix(~ superficie, new_dat)

# Calculer les valeur prédites
fitted_values <- mm %*% coef(mod_reg)

# Produire les erreurs pour l'intervalle de confiance
pvar <- diag(mm %*% tcrossprod(vcov(mod_reg), mm))
tvar <- pvar + var(resid(mod_reg))

# Générer une table pour produire facilement le graphique
table <- data.frame(
  superficie = new_dat$superficie,
  especes_y = fitted_values,
  # Intervalles de confiance
  lower_ci = fitted_values - 1.96 * sqrt(pvar),
  upper_ci = fitted_values + 1.96 * sqrt(pvar),
  # Intervalles de prédiction
  lower_pi = fitted_values - 1.96 * sqrt(tvar),
  upper_pi = fitted_values + 1.96 * sqrt(tvar)
)

```

On peut maintenant produire le graphique. Le code est gros mais il s'agit simplement d'ajouter des couches pour chaque information dans notre table de données.

```{r}
# Produire le graphique
ggplot(table) +

  # Les observations bruts
  geom_point(data = data_final,
             aes(x = superficie_cr, y = nbr_especes),
             shape = 19,
             color = "dodgerblue") +

  # La droite de la régression
  geom_line(aes(x = superficie, y = especes_y),
            linewidth = 1,
            color = "dodgerblue") +

  # Intervalles de confiance
  geom_line(aes(x = superficie, y = lower_ci),
            linetype = "dashed",
            linewidth = 1,
            color = "black") +
  geom_line(aes(x = superficie, y = upper_ci),
            linetype = "dashed",
            linewidth = 1,
            color = "black") +

  # Intervalles de prédiction
  geom_ribbon(aes(x = superficie,
                  ymin = lower_pi,
                  ymax = upper_pi),
              alpha = 0.2,
              fill = "dodgerblue") +

  # Si vous voulez jouer avec les valeurs des axes
  scale_x_continuous(breaks = seq(-2, 3, 1),
                     limits = c(-2, 3.6)) +

  # Quelques ajustements graphiques
  xlab("\nSuperficie normalisée") +
  ylab("Nombre d'espèces capturées\n") +
  theme_bw(base_size = 12) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12))


# 2. Méthode simple qui fait la même chose
# allez lire sur la fonction pour personnaliser le graph.
# grosso-modo, ça utilise ggplot2
# https://strengejacke.github.io/sjPlot/index.html
# install.packages("sjPlot")
#library(sjPlot)
#plot_model(modele1, type = "pred", terms = "Zsuperficie",
#           show.data = T, title = " ",
#           dot.size = 2, line.size = 1) +
#  scale_x_continuous(breaks = seq(-2, 3, 1),
#                     limits = c(-2, 3.6)) +
#  xlab("\nSuperficie normalisée") +
#  ylab("Nombre d'espèces capturées\n")


# On exporte la figure dans notre dossier "outputs"
#ggexport(graphique_regression, filename = "figure_regression.png",
#         width = 1500, height = 1500, res = 300)
```